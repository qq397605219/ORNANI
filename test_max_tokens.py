#!/usr/bin/env python3
"""
æµ‹è¯•åŠ¨æ€max_tokensåŠŸèƒ½çš„è„šæœ¬
"""

import requests
import json

# é…ç½®
BASE_URL = "http://localhost:8000"
API_TOKEN = "admin123"  # ä½¿ç”¨é…ç½®ä¸­çš„adminå¯†ç 

def test_dynamic_max_tokens():
    """æµ‹è¯•åŠ¨æ€max_tokensåŠŸèƒ½"""
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    # æµ‹è¯•ç”¨ä¾‹1: çŸ­æ¶ˆæ¯
    short_message_payload = {
        "model": "openai/gpt-oss-20b:free",
        "messages": [
            {"role": "user", "content": "Hello, how are you?"}
        ]
        # æ³¨æ„ï¼šæ²¡æœ‰æŒ‡å®šmax_tokensï¼Œåº”è¯¥ç”±ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—
    }
    
    # æµ‹è¯•ç”¨ä¾‹2: é•¿æ¶ˆæ¯
    long_message_payload = {
        "model": "openai/gpt-oss-20b:free", 
        "messages": [
            {"role": "user", "content": "Please write a detailed explanation about artificial intelligence, machine learning, deep learning, neural networks, and their applications in modern technology. Include examples and discuss the future prospects of AI development." * 10}
        ]
        # æ³¨æ„ï¼šæ²¡æœ‰æŒ‡å®šmax_tokensï¼Œåº”è¯¥ç”±ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—
    }
    
    print("ğŸ§ª æµ‹è¯•åŠ¨æ€max_tokensåŠŸèƒ½")
    print("=" * 50)
    
    # æµ‹è¯•çŸ­æ¶ˆæ¯
    print("\nğŸ“ æµ‹è¯•ç”¨ä¾‹1: çŸ­æ¶ˆæ¯")
    print(f"æ¶ˆæ¯å†…å®¹: {short_message_payload['messages'][0]['content']}")
    print(f"æ¶ˆæ¯é•¿åº¦: {len(short_message_payload['messages'][0]['content'])} å­—ç¬¦")
    
    try:
        response = requests.post(
            f"{BASE_URL}/v1/chat/completions",
            headers=headers,
            json=short_message_payload,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            usage = data.get("usage", {})
            print(f"âœ… è¯·æ±‚æˆåŠŸ")
            print(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
            print(f"   - è¾“å…¥tokens: {usage.get('prompt_tokens', 'N/A')}")
            print(f"   - è¾“å‡ºtokens: {usage.get('completion_tokens', 'N/A')}")
            print(f"   - æ€»tokens: {usage.get('total_tokens', 'N/A')}")
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
    
    # æµ‹è¯•é•¿æ¶ˆæ¯
    print("\nğŸ“ æµ‹è¯•ç”¨ä¾‹2: é•¿æ¶ˆæ¯")
    print(f"æ¶ˆæ¯é•¿åº¦: {len(long_message_payload['messages'][0]['content'])} å­—ç¬¦")
    
    try:
        response = requests.post(
            f"{BASE_URL}/v1/chat/completions",
            headers=headers,
            json=long_message_payload,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            usage = data.get("usage", {})
            print(f"âœ… è¯·æ±‚æˆåŠŸ")
            print(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
            print(f"   - è¾“å…¥tokens: {usage.get('prompt_tokens', 'N/A')}")
            print(f"   - è¾“å‡ºtokens: {usage.get('completion_tokens', 'N/A')}")
            print(f"   - æ€»tokens: {usage.get('total_tokens', 'N/A')}")
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

def test_model_context_info():
    """æµ‹è¯•æ¨¡å‹ä¸Šä¸‹æ–‡ä¿¡æ¯è·å–"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹ä¸Šä¸‹æ–‡ä¿¡æ¯")
    print("=" * 50)
    
    # ç›´æ¥æµ‹è¯•æ•°æ®åº“å‡½æ•°
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    
    from app import crud
    
    test_models = [
        "openai/gpt-oss-20b:free",
        "mistralai/mistral-small-3.2-24b-instruct:free",
        "google/gemma-3n-e2b-it:free"
    ]
    
    for model in test_models:
        context_length = crud.get_model_context_length(model)
        print(f"ğŸ“‹ æ¨¡å‹: {model}")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {context_length if context_length else 'æœªçŸ¥'}")

if __name__ == "__main__":
    test_model_context_info()
    test_dynamic_max_tokens()